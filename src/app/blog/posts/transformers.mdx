---
title: "Self attention and transformers"
publishedAt: "2023-02-19"
image: "/images/blogs_cover/optimus.jpg"
summary: This article is related to CS224n, Natural Language Processing with Deep learning from Standford. Today's lecture has enormous concepts and mathematical notations, which makes today the best and most attractive lecture so far in CS224n. (unfortunately, we didn't cover the proof of those mathematical assumptions in this note). We first take a look back at RNN models and what drawbacks it has. Then we slowly and firmly introduce a few techniques for solving RNN's drawbacks. Especially "ATTENTION". I will not spoil the rest, just leave those for you to discover. Eventually, we introduce the Transformer from "Attention is all you need", including its architecture, and mathematical calculation details. In the end, we talk about what the Transformer's drawbacks are.
tag: "Article"
---





![optimus prime](/images/blogs_cover/optimus.jpg)

<center>***Zesheng Jia***</center><br></br>

> Today's article has enormous concepts and mathematical notations, unfortunately, we didn't cover the proof of those mathematical assumptions of Transformer in this note. We first take a look back at RNN models and what drawbacks it has. Then we slowly and firmly introduce a few techniques for solving RNN's drawbacks. Especially "ATTENTION". I will not spoil the rest, just leave those for you to discover. Eventually, we introduce the Transformer from "Attention is all you need", including its architecture, mathematical calculation details. In the end, we talk about what the Transformer's drawbacks are. 

First, We take a brief look at Transformer's architecture. 


> Transformer from "Attention is all you need."


***Now, let's get started.***


**Table of Contents** 
- [Overview](#overview)
  - [1.RNN's Drawbacks and Transformer building blocks](#1rnns-drawbacks-and-transformer-building-blocks)
    - [1.1. Linear interaction distance](#11-linear-interaction-distance)
    - [1.2. Lack of parallelizability](#12-lack-of-parallelizability)
    - [1.3. Word windows](#13-word-windows)
    - [1.4. Attention](#14-attention)
    - [1.5. Self-Attention](#15-self-attention)
      - [1.5.1 Add sequence order to self attention](#151-add-sequence-order-to-self-attention)
      - [1.5.2 Sinusoidal position representations](#152-sinusoidal-position-representations)
        - [1.5.2.1 Positional encoding](#1521-positional-encoding)
      - [1.5.3 Adding nonliearities in self attention](#153-adding-nonliearities-in-self-attention)
      - [1.5.4 Masking the future in self-attention](#154-masking-the-future-in-self-attention)
    - [1.6. Summary of Self-attention building block:](#16-summary-of-self-attention-building-block)
  - [2. Introducing the Transformer model.](#2-introducing-the-transformer-model)
    - [2.1 Key-Query-Value Attention](#21-key-query-value-attention)
    - [2.2 Multi-headed attention](#22-multi-headed-attention)
    - [2.3 Residual connections](#23-residual-connections)
    - [2.4 Layer normalization](#24-layer-normalization)
    - [2.5 Scaled Dot Product](#25-scaled-dot-product)
    - [2.6 The Transformer Encoder - Decoder](#26-the-transformer-encoder---decoder)
      - [2.6.1 Encoder](#261-encoder)
      - [2.6.2 Decoder](#262-decoder)
  - [3. Great Results with Transformers](#3-great-results-with-transformers)
  - [4. Drawbacks and variants of Transformers](#4-drawbacks-and-variants-of-transformers)
    - [4.1 Quadratic computation as a function of sequence length](#41-quadratic-computation-as-a-function-of-sequence-length)


---- 
# Overview


1. From reccurrence RNN to attention - based NLP node_modules
2. Introducitng the Transformer model
3. Great results with Transformer 
4. Drawbacks and variants of transformers

## 1.RNN's Drawbacks and Transformer building blocks

### 1.1. Linear interaction distance

[Read more about this article in my blog website](https://ethansblog.vercel.app/blog/NLP/transformer/transformer_and_self_attention)

---
title: "Word vectors"
publishedAt: "2023-09-03"
image: "/images/blogs_cover/word_vectors.jpg"
summary: "This post will talk about word vectors in matrix designs, vector comparison, basic reweighting, dimensionality reduction, retrofitting and static representations. In first two chapters, we first take a close look of the benefits if we use word co-occurrence for sentiment analysis. Then we introduce the procedure of how to deal with a corpus from end to end. After that, we dive into the details of matrix designs. We introduce the different type of matrices we can use to represent words vs other objects. We also introduce the old NLP models that are about human-built feature functions. Then we talk about how to set up window size and scaling for co-occurrence detection and how those different settings will influence the results. In chapter 3, we...."
tag: "Article"
---



<center>***Zesheng Jia***</center><br></br>

<center>***[9275 words, 45mins]***</center><br></br>


> Today, we start a new course. This is a study note of CS224U, Natural Language Understanding from Stanford.
> 
> In this note, we will take a dive into how NLP implemented from details and code aspect. 
> 
> This post will talk about word vectors in matrix designs, vector comparison, basic reweighting, dimensionality reduction, retrofitting and static representations. 
> 
> In first two Chapters, we first take a close look of the benefits if we use word co-occurrence for sentiment analysis. Then we introduce the procedure of how to deal with a corpus from end to end. After that, we dive into the details of matrix designs. We introduce the different type of matrices we can use to represent words vs other objects. We also introduce the old NLP models that are about human built feature functions. Then we talk about how to set up window size and scaling size for co-occurrence detection and how those different settings will influence the results. 
> 
> In Chapter 3, we take a further step in Vector comparison aspect. By looking into real examples and discuss the influence of using different distance metrics. We introduce Euclidean distances, L2-length normalization, cosine distance, matching-based distances like Jaccard distance, Dice distance, overlap and KL divergence. Furthermore, we discuss what makes a properly defined distance metric. 


First, We take a brief look at t-SNE on word vectors.


# Table of contents:

- [Table of contents:](#table-of-contents)
  - [1. High-level goals and guiding hypotheses](#1-high-level-goals-and-guiding-hypotheses)
    - [1.1 Glimpse of word co-occurrence meaning](#11-glimpse-of-word-co-occurrence-meaning)
    - [1.2 High level goals](#12-high-level-goals)
    - [1.3 Great power, a great many design choices](#13-great-power-a-great-many-design-choices)
      - [1.3.1 Matrix design](#131-matrix-design)
      - [1.3.2 Reweighting](#132-reweighting)
      - [1.3.3 Dimensionality reduction](#133-dimensionality-reduction)
      - [1.3.4 Vector Comparison](#134-vector-comparison)
    - [1.4 Section 1's Reference](#14-section-1s-reference)
  - [2. Matrix designs](#2-matrix-designs)
    - [2.1 Types of Matrix Designs](#21-types-of-matrix-designs)
    - [2.2 Feature representations of data](#22-feature-representations-of-data)
    - [2.3 Windows and scaling: What is a co-occurrence](#23-windows-and-scaling-what-is-a-co-occurrence)
    - [2.4 Code](#24-code)
  - [3. Vector comparison](#3-vector-comparison)
    - [3.1 Running example](#31-running-example)
    - [3.2 Euclidean distances](#32-euclidean-distances)
    - [3.3 Length normalization](#33-length-normalization)
    - [3.4 Cosine distance](#34-cosine-distance)
    - [3.5 Matching-based methods](#35-matching-based-methods)
    - [3.6 KL divergence and variants](#36-kl-divergence-and-variants)
    - [3.7 Proper distance metric](#37-proper-distance-metric)
    - [3.8 Relationships and generalizations](#38-relationships-and-generalizations)
    - [3.9 CODES](#39-codes)
  - [4. Basic reweighting](#4-basic-reweighting)
    - [4.1 Goals of reweighting](#41-goals-of-reweighting)
    - [4.2 Normalization](#42-normalization)
    - [4.3 Observed/Expected](#43-observedexpected)
    - [$\star$ 4.4 Pointwise Mutual Information (PMI)](#star-44-pointwise-mutual-information-pmi)
    - [4.5 Positive PMI](#45-positive-pmi)
    - [4.6 Other weighting/normalization schemes](#46-other-weightingnormalization-schemes)
    - [4.7 Comparison between weight schemes](#47-comparison-between-weight-schemes)
    - [4.8 Relationships and generalizations](#48-relationships-and-generalizations)
    - [4.9 CODES](#49-codes)

## 1. High-level goals and guiding hypotheses

### 1.1 Glimpse of word co-occurrence meaning

This is a small fragment of a very large word by word co-occurrence matrix. The columns and rows are the same words representation. The objects are exactly the same vocabulary that is repeated across the columns and the cell values. We should start to get use to this kind of representation of words. Since, by this format of matrix, there could be meaning latent in such co-occurrence patterns. It may not obvious to us that we could extract anything about meaningful from this abstract matrix space. But we are going to see time and time again, this word by word matrix is actually a very powerful basis for developing meaning representations to start building intuition. 


[Read more about this article in my blog website](https://ethansblog.vercel.app/blog/NLP/word-vectors)